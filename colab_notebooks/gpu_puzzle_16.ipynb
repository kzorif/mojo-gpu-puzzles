{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22307,"status":"ok","timestamp":1754853567564,"user":{"displayName":"Mojonized","userId":"01435840102907113887"},"user_tz":-330},"id":"tDcsPkZEUmCd","outputId":"210e8a8f-ea2b-43f6-eefa-19fbb4a87452"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max==25.4.0\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from max==25.4.0) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from max==25.4.0) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.11/dist-packages (from max==25.4.0) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":["!pip install max==25.4.0 --index-url https://dl.modular.com/public/nightly/python/simple/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8429,"status":"ok","timestamp":1754853576005,"user":{"displayName":"Mojonized","userId":"01435840102907113887"},"user_tz":-330},"id":"T0dphloMUTnR","outputId":"eb92dcea-664b-4901-e59a-c933b7de6472"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 6121, done.\u001b[K\n","remote: Counting objects: 100% (433/433), done.\u001b[K\n","remote: Compressing objects: 100% (79/79), done.\u001b[K\n","remote: Total 6121 (delta 406), reused 354 (delta 354), pack-reused 5688 (from 3)\u001b[K\n","Receiving objects: 100% (6121/6121), 146.65 MiB | 24.52 MiB/s, done.\n","Resolving deltas: 100% (3804/3804), done.\n"]}],"source":["!git clone https://github.com/modular/mojo-gpu-puzzles"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1417,"status":"ok","timestamp":1754853577437,"user":{"displayName":"Mojonized","userId":"01435840102907113887"},"user_tz":-330},"id":"cvMe7KwFUbkR","outputId":"4b0dfb68-d4b2-4ee9-a99a-18354703e992"},"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.8 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}],"source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRVgLEGgUd47"},"outputs":[],"source":["import max.support.notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64xVp1e8UgLh"},"outputs":[],"source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UNKgFfbUpqj"},"outputs":[],"source":["mojo_code = \"\"\"\n","from sys import sizeof, argv\n","from testing import assert_equal\n","from gpu.host import DeviceContext\n","\n","# ANCHOR: naive_matmul\n","from gpu import thread_idx, block_idx, block_dim, barrier\n","from layout import Layout, LayoutTensor\n","from layout.tensor_builder import LayoutTensorBuild as tb\n","from gpu.memory import async_copy_wait_all\n","from layout.layout_tensor import copy_dram_to_sram_async\n","\n","alias TPB = 3\n","alias SIZE = 2\n","alias BLOCKS_PER_GRID = (1, 1)\n","alias THREADS_PER_BLOCK = (TPB, TPB)\n","alias dtype = DType.float32\n","alias layout = Layout.row_major(SIZE, SIZE)\n","\n","\n","fn naive_matmul[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    a: LayoutTensor[mut=False, dtype, layout],\n","    b: LayoutTensor[mut=False, dtype, layout],\n","):\n","    row = block_dim.y * block_idx.y + thread_idx.y\n","    col = block_dim.x * block_idx.x + thread_idx.x\n","    # FILL ME IN (roughly 6 lines)\n","\n","    if row < size and col < size:\n","      var acc: output.element_type = 0\n","\n","      @parameter\n","      for k in range(size):\n","        acc += a[row, k] * b[k, col]\n","\n","      output[row, col] = acc\n","\n","\n","# ANCHOR_END: naive_matmul\n","\n","\n","# ANCHOR: single_block_matmul\n","fn single_block_matmul[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    a: LayoutTensor[mut=False, dtype, layout],\n","    b: LayoutTensor[mut=False, dtype, layout],\n","):\n","    row = block_dim.y * block_idx.y + thread_idx.y\n","    col = block_dim.x * block_idx.x + thread_idx.x\n","    local_row = thread_idx.y\n","    local_col = thread_idx.x\n","    # FILL ME IN (roughly 12 lines)\n","\n","    shared_a = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n","    shared_b = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n","\n","    if row<size and col <size:\n","      shared_a[local_row, local_col] = a[row, col]\n","      shared_b[local_row, local_col] = b[row, col]\n","\n","    barrier()\n","\n","    if row < size and col < size:\n","      var acc: output.element_type = 0\n","\n","      @parameter\n","      for k in range(size):\n","        acc += shared_a[local_row, k] * shared_b[k, local_col]\n","\n","      output[row, col] = acc\n","# ANCHOR_END: single_block_matmul\n","\n","# ANCHOR: matmul_tiled\n","alias SIZE_TILED = 9\n","alias BLOCKS_PER_GRID_TILED = (3, 3)  # each block convers 3x3 elements\n","alias THREADS_PER_BLOCK_TILED = (TPB, TPB)\n","alias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)\n","\n","\n","fn matmul_tiled[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    a: LayoutTensor[mut=False, dtype, layout],\n","    b: LayoutTensor[mut=False, dtype, layout],\n","):\n","    local_row = thread_idx.y\n","    local_col = thread_idx.x\n","    tiled_row = block_idx.y * TPB + thread_idx.y\n","    tiled_col = block_idx.x * TPB + thread_idx.x\n","\n","    # FILL ME IN (roughly 20 lines)\n","    out_tile = output.tile[TPB, TPB](block_idx.y, block_idx.x)\n","    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)\n","    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)\n","\n","    var acc: output.element_type = 0\n","\n","    alias load_a_layout = Layout.row_major(1, TPB)\n","    alias load_b_layout = Layout.row_major(1, TPB)\n","\n","    @parameter\n","    for idx in range(size // TPB):\n","      a_tile = a.tile[TPB, TPB](block_idx.y, idx)\n","      b_tile = b.tile[TPB, TPB](idx, block_idx.x)\n","\n","      copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)\n","      copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)\n","\n","      async_copy_wait_all()\n","      barrier()\n","\n","      @parameter\n","      for k in range(TPB):\n","        acc += a_shared[local_row, k] * b_shared[k ,local_col]\n","\n","      barrier()\n","\n","    if tiled_row < size and tiled_col < size:\n","      out_tile[local_row, local_col] = acc\n","\n","\n","# ANCHOR_END: matmul_tiled\n","\n","\n","def main():\n","    with DeviceContext() as ctx:\n","        if len(argv()) != 2 or argv()[1] not in [\n","            \"--naive\",\n","            \"--single-block\",\n","            \"--tiled\",\n","        ]:\n","            raise Error(\n","                \"Expected one argument: '--naive', '--single-block', or\"\n","                \" '--tiled'\"\n","            )\n","        size = SIZE_TILED if argv()[1] == \"--tiled\" else SIZE\n","        out = ctx.enqueue_create_buffer[dtype](size * size).enqueue_fill(0)\n","        inp1 = ctx.enqueue_create_buffer[dtype](size * size).enqueue_fill(0)\n","        inp2 = ctx.enqueue_create_buffer[dtype](size * size).enqueue_fill(0)\n","        expected = ctx.enqueue_create_host_buffer[dtype](\n","            size * size\n","        ).enqueue_fill(0)\n","        with inp1.map_to_host() as inp1_host, inp2.map_to_host() as inp2_host:\n","            for row in range(size):\n","                for col in range(size):\n","                    val = row * size + col\n","                    # row major: placing elements row by row\n","                    inp1_host[row * size + col] = val\n","                    inp2_host[row * size + col] = Float32(2.0) * val\n","\n","            # inp1 @ inp2.T\n","            for i in range(size):\n","                for j in range(size):\n","                    for k in range(size):\n","                        expected[i * size + j] += (\n","                            inp1_host[i * size + k] * inp2_host[k * size + j]\n","                        )\n","\n","        out_tensor = LayoutTensor[mut=False, dtype, layout](out.unsafe_ptr())\n","        a_tensor = LayoutTensor[mut=False, dtype, layout](inp1.unsafe_ptr())\n","        b_tensor = LayoutTensor[mut=False, dtype, layout](inp2.unsafe_ptr())\n","\n","        if argv()[1] == \"--naive\":\n","            ctx.enqueue_function[naive_matmul[layout, SIZE]](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                grid_dim=BLOCKS_PER_GRID,\n","                block_dim=THREADS_PER_BLOCK,\n","            )\n","        elif argv()[1] == \"--single-block\":\n","            ctx.enqueue_function[single_block_matmul[layout, SIZE]](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                grid_dim=BLOCKS_PER_GRID,\n","                block_dim=THREADS_PER_BLOCK,\n","            )\n","        elif argv()[1] == \"--tiled\":\n","            # Need to update the layout of the tensors to the tiled layout\n","            out_tensor_tiled = LayoutTensor[mut=False, dtype, layout_tiled](\n","                out.unsafe_ptr()\n","            )\n","            a_tensor_tiled = LayoutTensor[mut=False, dtype, layout_tiled](\n","                inp1.unsafe_ptr()\n","            )\n","            b_tensor_tiled = LayoutTensor[mut=False, dtype, layout_tiled](\n","                inp2.unsafe_ptr()\n","            )\n","\n","            ctx.enqueue_function[matmul_tiled[layout_tiled, SIZE_TILED]](\n","                out_tensor_tiled,\n","                a_tensor_tiled,\n","                b_tensor_tiled,\n","                grid_dim=BLOCKS_PER_GRID_TILED,\n","                block_dim=THREADS_PER_BLOCK_TILED,\n","            )\n","\n","        ctx.synchronize()\n","\n","        with out.map_to_host() as out_host:\n","            print(\"out:\", out_host)\n","            print(\"expected:\", expected)\n","            for col in range(size):\n","                for row in range(size):\n","                    assert_equal(\n","                        out_host[col * size + row], expected[col * size + row]\n","                    )\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QSJEeGDUuUN"},"outputs":[],"source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p16/p16.mojo\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3KEFkpjUuWH","executionInfo":{"status":"ok","timestamp":1754854003421,"user_tz":-330,"elapsed":11677,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"c9fa5513-c256-41a1-a7f0-dcfc53a42e4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo problems/p16/p16.mojo --single-block\u001b[0m\n","\u001b[1m/content/mojo-gpu-puzzles/problems/p16/p16.mojo:92:27: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1massignment to 'local_row' was never used; assign to '_' instead?\n","\u001b[0m    local_row = thread_idx.y\n","\u001b[0;1;32m                          ^\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p16/p16.mojo:93:27: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1massignment to 'local_col' was never used; assign to '_' instead?\n","\u001b[0m    local_col = thread_idx.x\n","\u001b[0;1;32m                          ^\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p16/p16.mojo:94:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1massignment to 'tiled_row' was never used; assign to '_' instead?\n","\u001b[0m    tiled_row = block_idx.y * TPB + thread_idx.y\n","\u001b[0;1;32m                                  ^\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p16/p16.mojo:95:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1massignment to 'tiled_col' was never used; assign to '_' instead?\n","\u001b[0m    tiled_col = block_idx.x * TPB + thread_idx.x\n","\u001b[0;1;32m                                  ^\n","\u001b[0mout: HostBuffer([4.0, 6.0, 12.0, 22.0])\n","expected: HostBuffer([4.0, 6.0, 12.0, 22.0])\n"]}],"source":["!cd /content/mojo-gpu-puzzles && uv run poe p16 --single-block"]},{"cell_type":"code","source":[],"metadata":{"id":"DXKaVmwjY2Tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzjNAcyrU91S"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOk0bwB7kn4uMF7ordOfPAv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}