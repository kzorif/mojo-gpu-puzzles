{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN6fVg6P/WUu2j2HW5NkF6H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16BuqOB07qRK","executionInfo":{"status":"ok","timestamp":1756828992369,"user_tz":-330,"elapsed":26617,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"e76289c0-a897-4263-a6f7-741e4fbe7fcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max==25.4.0\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":["!pip install max==25.4.0 --index-url https://dl.modular.com/public/nightly/python/simple/"]},{"cell_type":"code","source":["!git clone https://github.com/modular/mojo-gpu-puzzles"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbFYA80K74La","executionInfo":{"status":"ok","timestamp":1756829002481,"user_tz":-330,"elapsed":10114,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"c8aa753a-c4da-44b9-d23b-8aa0f88ca1c2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 6332, done.\u001b[K\n","remote: Counting objects: 100% (481/481), done.\u001b[K\n","remote: Compressing objects: 100% (65/65), done.\u001b[K\n","remote: Total 6332 (delta 448), reused 416 (delta 416), pack-reused 5851 (from 3)\u001b[K\n","Receiving objects: 100% (6332/6332), 148.65 MiB | 19.48 MiB/s, done.\n","Resolving deltas: 100% (3922/3922), done.\n"]}]},{"cell_type":"code","source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJfr7_t275kZ","executionInfo":{"status":"ok","timestamp":1756829004588,"user_tz":-330,"elapsed":2096,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"fea9ddc2-8a54-4fbc-c589-805e5e9f9978"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.14 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}]},{"cell_type":"code","source":["import max.support.notebook"],"metadata":{"id":"d5awi7GC77Cn","executionInfo":{"status":"ok","timestamp":1756829004622,"user_tz":-330,"elapsed":23,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"],"metadata":{"id":"MWCgz6DQ78t5","executionInfo":{"status":"ok","timestamp":1756829004638,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["mojo_code = \"\"\"\n","from gpu import thread_idx, block_idx, block_dim, grid_dim, barrier\n","from os.atomic import Atomic\n","from gpu.warp import WARP_SIZE\n","from gpu import block\n","from gpu.host import DeviceContext\n","from layout import Layout, LayoutTensor\n","from layout.tensor_builder import LayoutTensorBuild as tb\n","from sys import argv\n","from testing import assert_equal\n","from math import floor\n","\n","\n","# ANCHOR: block_sum_dot_product\n","alias SIZE = 128\n","alias TPB = 128\n","alias NUM_BINS = 8\n","alias in_layout = Layout.row_major(SIZE)\n","alias out_layout = Layout.row_major(1)\n","alias dtype = DType.float32\n","\n","fn traditional_dot_product[\n","    in_layout: Layout, out_layout: Layout, tpb: Int\n","](\n","    output: LayoutTensor[mut=True, dtype, out_layout],\n","    a: LayoutTensor[mut=False, dtype, in_layout],\n","    b: LayoutTensor[mut=False, dtype, in_layout],\n","    size: Int,\n","):\n","\n","    shared = tb[dtype]().row_major[tpb]().shared().alloc()\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    # Each thread computes partial product\n","    if global_i < size:\n","        a_val = rebind[Scalar[dtype]](a[global_i])\n","        b_val = rebind[Scalar[dtype]](b[global_i])\n","        shared[local_i] = a_val * b_val\n","\n","    barrier()\n","\n","    # Tree reduction in shared memory - complex but educational\n","    var stride = tpb // 2\n","    while stride > 0:\n","        if local_i < stride:\n","            shared[local_i] += shared[local_i + stride]\n","        barrier()\n","        stride //= 2\n","\n","    # Only thread 0 writes final result\n","    if local_i == 0:\n","        output[0] = shared[0]\n","\n","fn block_sum_dot_product[\n","    in_layout: Layout, out_layout: Layout, tpb: Int\n","](\n","    output: LayoutTensor[mut=True, dtype, out_layout],\n","    a: LayoutTensor[mut=False, dtype, in_layout],\n","    b: LayoutTensor[mut=False, dtype, in_layout],\n","    size: Int,\n","):\n","\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    # FILL IN (roughly 6 lines)\n","    var partial_product: Scalar[dtype] = 0.0\n","    if global_i < size:\n","        partial_product = a[global_i][0] * b[global_i][0]\n","    total = block.sum[block_size=tpb, broadcast=False](val=SIMD[DType.float32, 1](partial_product))\n","    if local_i == 0:\n","        output[0] = total[0]\n","\n","# ANCHOR_END: block_sum_dot_product\n","\n","# ANCHOR: block_histogram\n","alias bin_layout = Layout.row_major(SIZE)  # Max SIZE elements per bin\n","\n","\n","fn block_histogram_bin_extract[\n","    in_layout: Layout, bin_layout: Layout, out_layout: Layout, tpb: Int\n","](\n","    input_data: LayoutTensor[mut=False, dtype, in_layout],\n","    bin_output: LayoutTensor[mut=True, dtype, bin_layout],\n","    count_output: LayoutTensor[mut=True, DType.int32, out_layout],\n","    size: Int,\n","    target_bin: Int,\n","    num_bins: Int,\n","):\n","\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    # Step 1: Each thread determines its bin and element value\n","    # FILL IN (roughly 9 lines)\n","    var my_value: Scalar[dtype] = 0.0\n","    var my_bin: Int = -1\n","\n","    if global_i < size:\n","        my_value = input_data[global_i][0]\n","        my_bin = Int(floor(my_value * num_bins))\n","        if my_bin >= num_bins:\n","            my_bin = num_bins - 1\n","        if my_bin < 0:\n","            my_bin = 0\n","\n","    # Step 2: Create predicate for target bin extraction\n","    # FILL IN (roughly 3 line)\n","    var belongs_to_target: Int = 0\n","    if global_i < size and my_bin == target_bin:\n","        belongs_to_target = 1\n","\n","    # Step 3: Use block.prefix_sum() for parallel bin extraction!\n","    # This computes where each thread should write within the target bin\n","    # FILL IN (1 line)\n","    write_offset = block.prefix_sum[dtype = DType.int32, block_size=tpb, exclusive=True](val=SIMD[DType.int32, 1](belongs_to_target))\n","\n","    # Step 4: Extract and pack elements belonging to target_bin\n","    # FILL IN (roughly 2 line)\n","    if belongs_to_target == 1:\n","        bin_output[Int(write_offset[0])] = my_value\n","\n","    # Step 5: Final thread computes total count for this bin\n","    # FILL IN (roughly 3 line)\n","    if local_i == tpb - 1:\n","        total_count = write_offset[0] + belongs_to_target\n","        count_output[0] = total_count\n","\n","\n","# ANCHOR_END: block_histogram\n","\n","# ANCHOR: block_normalize\n","\n","alias vector_layout = Layout.row_major(SIZE)\n","\n","\n","fn block_normalize_vector[\n","    in_layout: Layout, out_layout: Layout, tpb: Int\n","](\n","    input_data: LayoutTensor[mut=False, dtype, in_layout],\n","    output_data: LayoutTensor[mut=True, dtype, out_layout],\n","    size: Int,\n","):\n","\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    # Step 1: Each thread loads its element\n","    # FILL IN (roughly 3 lines)\n","    var my_value: Scalar[dtype] = 0.0\n","    if global_i < size:\n","        my_value = input_data[global_i][0]\n","\n","    # Step 2: Use block.sum() to compute total sum (familiar from earlier!)\n","    # FILL IN (1 line)\n","    total_sum = block.sum[block_size=tpb, broadcast=False](val=SIMD[DType.float32, 1](my_value))\n","\n","    # Step 3: Thread 0 computes mean value\n","    # FILL IN (roughly 4 lines)\n","    var mean_value: Scalar[dtype] = 1.0\n","    if local_i == 0:\n","        if total_sum[0] > 0.0:\n","            mean_value = total_sum[0] / Float32(size)\n","\n","    # Step 4: block.broadcast() shares mean to ALL threads!\n","    # This completes the block operations trilogy demonstration\n","    # FILL IN (1 line)\n","    broadcast_mean = block.broadcast[dtype=DType.float32, width=1, block_size=tpb](val=SIMD[DType.float32, 1](mean_value), src_thread=UInt(0))\n","\n","    # Step 5: Each thread normalizes by the mean\n","    # FILL IN (roughly 3 lines)\n","    if global_i < size:\n","        normalized_value = my_value / broadcast_mean[0]\n","        output_data[global_i] = normalized_value\n","\n","# ANCHOR_END: block_normalize\n","\n","\n","def main():\n","    if len(argv()) != 2:\n","        print(\n","            \"Usage: --traditional-dot-product | --block-sum-dot-product |\"\n","            \" --histogram | --normalize\"\n","        )\n","        return\n","\n","    with DeviceContext() as ctx:\n","        if argv()[1] == \"--traditional-dot-product\":\n","            out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","            a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","            b_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","            var expected: Scalar[dtype] = 0.0\n","            with a.map_to_host() as a_host, b_buf.map_to_host() as b_host:\n","                for i in range(SIZE):\n","                    a_host[i] = i\n","                    b_host[i] = 2 * i\n","                    expected += a_host[i] * b_host[i]\n","\n","            print(\"SIZE:\", SIZE)\n","            print(\"TPB:\", TPB)\n","            print(\"Expected result:\", expected)\n","\n","            a_tensor = LayoutTensor[mut=False, dtype, in_layout](a.unsafe_ptr())\n","            b_tensor = LayoutTensor[mut=False, dtype, in_layout](\n","                b_buf.unsafe_ptr()\n","            )\n","            out_tensor = LayoutTensor[mut=True, dtype, out_layout](\n","                out.unsafe_ptr()\n","            )\n","\n","            # Traditional approach: works perfectly when size == TPB\n","            ctx.enqueue_function[\n","                traditional_dot_product[in_layout, out_layout, TPB]\n","            ](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                SIZE,\n","                grid_dim=(1, 1),  # ✅ Single block works when size == TPB\n","                block_dim=(TPB, 1),\n","            )\n","\n","            ctx.synchronize()\n","\n","            with out.map_to_host() as result_host:\n","                result = result_host[0]\n","                print(\"Traditional result:\", result)\n","                assert_equal(result, expected)\n","                print(\"Complex: shared memory + barriers + tree reduction\")\n","\n","        elif argv()[1] == \"--block-sum-dot-product\":\n","            out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","            a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","            b_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","            var expected: Scalar[dtype] = 0.0\n","            with a.map_to_host() as a_host, b_buf.map_to_host() as b_host:\n","                for i in range(SIZE):\n","                    a_host[i] = i\n","                    b_host[i] = 2 * i\n","                    expected += a_host[i] * b_host[i]\n","\n","            print(\"SIZE:\", SIZE)\n","            print(\"TPB:\", TPB)\n","            print(\"Expected result:\", expected)\n","\n","            a_tensor = LayoutTensor[mut=False, dtype, in_layout](a.unsafe_ptr())\n","            b_tensor = LayoutTensor[mut=False, dtype, in_layout](\n","                b_buf.unsafe_ptr()\n","            )\n","            out_tensor = LayoutTensor[mut=True, dtype, out_layout](\n","                out.unsafe_ptr()\n","            )\n","\n","            # Block.sum(): Same result with dramatically simpler code!\n","            ctx.enqueue_function[\n","                block_sum_dot_product[in_layout, out_layout, TPB]\n","            ](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                SIZE,\n","                grid_dim=(1, 1),  # Same single block as traditional\n","                block_dim=(TPB, 1),\n","            )\n","\n","            ctx.synchronize()\n","\n","            with out.map_to_host() as result_host:\n","                result = result_host[0]\n","                print(\"Block.sum result:\", result)\n","                assert_equal(result, expected)\n","                print(\"Block.sum() gives identical results!\")\n","                print(\n","                    \"Compare the code: 15+ lines of barriers → 1 line of\"\n","                    \" block.sum()!\"\n","                )\n","                print(\"Just like warp.sum() but for the entire block\")\n","\n","        elif argv()[1] == \"--histogram\":\n","            print(\"SIZE:\", SIZE)\n","            print(\"TPB:\", TPB)\n","            print(\"NUM_BINS:\", NUM_BINS)\n","            print()\n","\n","            # Create input data with known distribution across bins\n","            input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","            # Create test data: values distributed across 8 bins [0.0, 1.0)\n","            with input_buf.map_to_host() as input_host:\n","                for i in range(SIZE):\n","                    # Create values: 0.1, 0.2, 0.3, ..., cycling through bins\n","                    input_host[i] = (\n","                        Float32(i % 80) / 100.0\n","                    )  # Values [0.0, 0.79]\n","\n","            print(\"Input sample:\", end=\" \")\n","            with input_buf.map_to_host() as input_host:\n","                for i in range(min(16, SIZE)):\n","                    print(input_host[i], end=\" \")\n","            print(\"...\")\n","            print()\n","\n","            input_tensor = LayoutTensor[mut=False, dtype, in_layout](\n","                input_buf.unsafe_ptr()\n","            )\n","\n","            # Demonstrate histogram for each bin using block.prefix_sum()\n","            for target_bin in range(NUM_BINS):\n","                print(\n","                    \"=== Processing Bin\",\n","                    target_bin,\n","                    \"(range [\",\n","                    Float32(target_bin) / NUM_BINS,\n","                    \",\",\n","                    Float32(target_bin + 1) / NUM_BINS,\n","                    \")) ===\",\n","                )\n","\n","                # Create output buffers for this bin\n","                bin_data = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(\n","                    0\n","                )\n","                bin_count = ctx.enqueue_create_buffer[DType.int32](\n","                    1\n","                ).enqueue_fill(0)\n","\n","                bin_tensor = LayoutTensor[mut=True, dtype, bin_layout](\n","                    bin_data.unsafe_ptr()\n","                )\n","                count_tensor = LayoutTensor[mut=True, DType.int32, out_layout](\n","                    bin_count.unsafe_ptr()\n","                )\n","\n","                # Execute histogram kernel for this specific bin\n","                ctx.enqueue_function[\n","                    block_histogram_bin_extract[\n","                        in_layout, bin_layout, out_layout, TPB\n","                    ]\n","                ](\n","                    input_tensor,\n","                    bin_tensor,\n","                    count_tensor,\n","                    SIZE,\n","                    target_bin,\n","                    NUM_BINS,\n","                    grid_dim=(\n","                        1,\n","                        1,\n","                    ),  # Single block demonstrates block.prefix_sum()\n","                    block_dim=(TPB, 1),\n","                )\n","\n","                ctx.synchronize()\n","\n","                # Display results for this bin\n","                with bin_count.map_to_host() as count_host:\n","                    count = count_host[0]\n","                    print(\"Bin\", target_bin, \"count:\", count)\n","\n","                with bin_data.map_to_host() as bin_host:\n","                    print(\"Bin\", target_bin, \"extracted elements:\", end=\" \")\n","                    for i in range(min(8, Int(count))):\n","                        print(bin_host[i], end=\" \")\n","                    if count > 8:\n","                        print(\"...\")\n","                    else:\n","                        print()\n","                print()\n","\n","        elif argv()[1] == \"--normalize\":\n","            print(\"SIZE:\", SIZE)\n","            print(\"TPB:\", TPB)\n","            print()\n","\n","            # Create input data with known values for easy verification\n","            input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","            output_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","            # Create test data: values like [1, 2, 3, 4, 5, ..., 8, 1, 2, 3, ...]\n","            # Mean value will be 4.5, so normalized values will be input[i] / 4.5\n","            var sum_value: Scalar[dtype] = 0.0\n","            with input_buf.map_to_host() as input_host:\n","                for i in range(SIZE):\n","                    # Create values cycling 1-8, mean will be 4.5\n","                    value = Float32(\n","                        (i % 8) + 1\n","                    )  # Values 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, ...\n","                    input_host[i] = value\n","                    sum_value += value\n","\n","            var mean_value = sum_value / Float32(SIZE)\n","\n","            print(\"Input sample:\", end=\" \")\n","            with input_buf.map_to_host() as input_host:\n","                for i in range(min(16, SIZE)):\n","                    print(input_host[i], end=\" \")\n","            print(\"...\")\n","            print(\"Sum value:\", sum_value)\n","            print(\"Mean value:\", mean_value)\n","            print()\n","\n","            input_tensor = LayoutTensor[mut=False, dtype, in_layout](\n","                input_buf.unsafe_ptr()\n","            )\n","            output_tensor = LayoutTensor[mut=True, dtype, vector_layout](\n","                output_buf.unsafe_ptr()\n","            )\n","\n","            # Execute vector normalization kernel\n","            ctx.enqueue_function[\n","                block_normalize_vector[in_layout, vector_layout, TPB]\n","            ](\n","                input_tensor,\n","                output_tensor,\n","                SIZE,\n","                grid_dim=(1, 1),  # Single block demonstrates block.broadcast()\n","                block_dim=(TPB, 1),\n","            )\n","\n","            ctx.synchronize()\n","\n","            # Verify results\n","            print(\"Mean Normalization Results:\")\n","            with output_buf.map_to_host() as output_host:\n","                print(\"Normalized sample:\", end=\" \")\n","                for i in range(min(16, SIZE)):\n","                    print(output_host[i], end=\" \")\n","                print(\"...\")\n","\n","                # Verify that the mean normalization worked (mean of output should be ~1.0)\n","                var output_sum: Scalar[dtype] = 0.0\n","                for i in range(SIZE):\n","                    output_sum += output_host[i]\n","\n","                var output_mean = output_sum / Float32(SIZE)\n","                print(\"Output sum:\", output_sum)\n","                print(\"Output mean:\", output_mean)\n","                print(\n","                    \"✅ Success: Output mean is\",\n","                    output_mean,\n","                    \"(should be close to 1.0)\",\n","                )\n","        else:\n","            print(\n","                \"Available options: [--traditional-dot-product |\"\n","                \" --block-sum-dot-product | --histogram | --normalize]\"\n","            )\n","\n","\"\"\""],"metadata":{"id":"NC1dHcaI7-Qg","executionInfo":{"status":"ok","timestamp":1756830007813,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p27/p27.mojo\")"],"metadata":{"id":"IZpK81hX7-SX","executionInfo":{"status":"ok","timestamp":1756830007976,"user_tz":-330,"elapsed":13,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["!cd /content/mojo-gpu-puzzles && uv run poe p27 --normalize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CA9VUkUJ7-UP","executionInfo":{"status":"ok","timestamp":1756830019240,"user_tz":-330,"elapsed":11082,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"4be1f7e1-827f-42af-ec59-28479e8f29ff"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo problems/p27/p27.mojo --normalize\u001b[0m\n","SIZE: 128\n","TPB: 128\n","\n","Input sample: 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 ...\n","Sum value: 576.0\n","Mean value: 4.5\n","\n","Mean Normalization Results:\n","Normalized sample: 0.22222222 0.44444445 0.6666667 0.8888889 1.1111112 1.3333334 1.5555556 1.7777778 0.22222222 0.44444445 0.6666667 0.8888889 1.1111112 1.3333334 1.5555556 1.7777778 ...\n","Output sum: 128.0\n","Output mean: 1.0\n","✅ Success: Output mean is 1.0 (should be close to 1.0)\n"]}]},{"cell_type":"code","source":["!uv run mojo --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7EM7uhh27-WE","executionInfo":{"status":"ok","timestamp":1756829106399,"user_tz":-330,"elapsed":607,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"14f930ea-2bd4-485d-f396-fb8bbb9e8868"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mojo 25.4.0 (fbeca2fa)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"U_BnAgnt7-YK","executionInfo":{"status":"ok","timestamp":1756829106404,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":11,"outputs":[]}]}