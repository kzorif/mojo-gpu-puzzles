{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNiq3XOxg+xTzuI+SAhv/PE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbLbzim508bV","executionInfo":{"status":"ok","timestamp":1756557714183,"user_tz":-330,"elapsed":32021,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"a2db51d6-3a6d-4548-9a18-6d9896cb71d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max==25.4.0\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":["!pip install max==25.4.0 --index-url https://dl.modular.com/public/nightly/python/simple/"]},{"cell_type":"code","source":["!git clone https://github.com/modular/mojo-gpu-puzzles"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c6a-PXH1ANX","executionInfo":{"status":"ok","timestamp":1756557721106,"user_tz":-330,"elapsed":6921,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"d5de5bb1-6ed7-4e67-fc3c-a8f5fa86c329"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 6332, done.\u001b[K\n","remote: Counting objects: 100% (481/481), done.\u001b[K\n","remote: Compressing objects: 100% (65/65), done.\u001b[K\n","remote: Total 6332 (delta 449), reused 416 (delta 416), pack-reused 5851 (from 3)\u001b[K\n","Receiving objects: 100% (6332/6332), 148.65 MiB | 32.39 MiB/s, done.\n","Resolving deltas: 100% (3923/3923), done.\n"]}]},{"cell_type":"code","source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8-aLScI1DBV","executionInfo":{"status":"ok","timestamp":1756557722722,"user_tz":-330,"elapsed":1609,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"1ae65b77-889b-4bb8-e4a6-7df518e739ba"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.14 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}]},{"cell_type":"code","source":["import max.support.notebook"],"metadata":{"id":"8HUjwytp1Er3","executionInfo":{"status":"ok","timestamp":1756557722735,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"],"metadata":{"id":"BdrGe6v51GRp","executionInfo":{"status":"ok","timestamp":1756557722748,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["mojo_code = \"\"\"\n","from memory import UnsafePointer\n","from gpu import thread_idx, block_idx, block_dim, barrier\n","from gpu.host import DeviceContext, HostBuffer, DeviceBuffer\n","from layout import Layout, LayoutTensor\n","from layout.tensor_builder import LayoutTensorBuild as tb\n","from math import exp\n","from utils.numerics import max_finite, min_finite\n","import compiler\n","from runtime.asyncrt import DeviceContextPtr\n","from tensor import InputTensor, OutputTensor\n","from gpu.memory import async_copy_wait_all\n","from layout.layout_tensor import copy_dram_to_sram_async\n","\n","alias SEQ_LEN = 16\n","alias D = 16\n","alias TPB = SEQ_LEN\n","\n","\n","# Tiled matrix multiplication from p14 - adapted for attention\n","fn matmul_idiomatic_tiled[\n","    layout: Layout,\n","    rows: Int,\n","    cols: Int,\n","    inner: Int,\n","    dtype: DType = DType.float32,\n","](\n","    output: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","    a: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","    b: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","):\n","    local_row = thread_idx.y\n","    local_col = thread_idx.x\n","    tiled_row = block_idx.y * TPB + local_row\n","    tiled_col = block_idx.x * TPB + local_col\n","\n","    # Get the tile of the output matrix that this thread block is responsible for\n","    out_tile = output.tile[TPB, TPB](block_idx.y, block_idx.x)\n","    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)\n","    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)\n","\n","    var acc: output.element_type = 0\n","\n","    alias load_a_layout = Layout.row_major(1, TPB)\n","    alias load_b_layout = Layout.row_major(TPB, 1)\n","\n","    for idx in range((inner + TPB - 1) // TPB):\n","        # Get tiles from A and B matrices\n","        a_tile = a.tile[TPB, TPB](block_idx.y, idx)\n","        b_tile = b.tile[TPB, TPB](idx, block_idx.x)\n","\n","        # Asynchronously copy tiles to shared memory\n","        copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)\n","        copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)\n","\n","        # Wait for all async copies to complete\n","        async_copy_wait_all()\n","        barrier()\n","\n","        # Compute partial matrix multiplication for this tile\n","        @parameter\n","        for k in range(TPB):\n","            acc += a_shared[local_row, k] * b_shared[k, local_col]\n","\n","        barrier()\n","\n","    # Write final result with bounds checking (needed for attention's variable sizes)\n","    if tiled_row < rows and tiled_col < cols:\n","        out_tile[local_row, local_col] = acc\n","\n","\n","# ANCHOR: transpose_kernel\n","fn transpose_kernel[\n","    layout_in: Layout,  # Layout for input matrix (seq_len, d)\n","    layout_out: Layout,  # Layout for output matrix (d, seq_len)\n","    rows: Int,\n","    cols: Int,\n","    dtype: DType = DType.float32,\n","](\n","    output: LayoutTensor[mut=True, dtype, layout_out, MutableAnyOrigin],\n","    inp: LayoutTensor[mut=False, dtype, layout_in, MutableAnyOrigin],\n","):\n","    # FILL ME IN (roughly 18 lines)\n","    shared_tile = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n","\n","    local_row = thread_idx.y\n","    local_col = thread_idx.x\n","\n","    global_row = block_idx.y * TPB + local_row\n","    global_col = block_idx.x * TPB + local_col\n","\n","    if global_row < rows and global_col < cols:\n","      shared_tile[local_row, local_col] = inp[global_row, global_col]\n","    else:\n","      shared_tile[local_row, local_col] = 0.0\n","\n","    barrier()\n","\n","    out_row = block_idx.x * TPB + local_row\n","    out_col = block_idx.y * TPB + local_col\n","\n","    if out_row < cols and out_col < rows:\n","      output[out_row, out_col] = shared_tile[local_col, local_row]\n","\n","\n","# ANCHOR_END: transpose_kernel\n","\n","\n","# Apply softmax to attention scores taken from p16\n","fn softmax_gpu_kernel[\n","    layout: Layout,\n","    seq_len: Int,\n","    dtype: DType = DType.float32,\n","](\n","    output: LayoutTensor[mut=True, dtype, layout, MutableAnyOrigin],\n","    scores: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","):\n","    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()\n","    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    var thread_max: Scalar[dtype] = min_finite[dtype]()\n","    if global_i < seq_len:\n","        thread_max = rebind[Scalar[dtype]](scores[global_i])\n","\n","    shared_max[local_i] = thread_max\n","    barrier()\n","\n","    # Parallel reduction to find max\n","    stride = TPB // 2\n","    while stride > 0:\n","        if local_i < stride:\n","            shared_max[local_i] = max(\n","                shared_max[local_i], shared_max[local_i + stride]\n","            )\n","        barrier()\n","        stride = stride // 2\n","\n","    block_max = shared_max[0]\n","\n","    var exp_val: Scalar[dtype] = 0.0\n","    if global_i < seq_len:\n","        exp_val = rebind[Scalar[dtype]](exp(scores[global_i] - block_max))\n","        output[global_i] = exp_val\n","\n","    shared_sum[local_i] = exp_val\n","    barrier()\n","\n","    # Parallel reduction for sum\n","    stride = TPB // 2\n","    while stride > 0:\n","        if local_i < stride:\n","            shared_sum[local_i] = (\n","                shared_sum[local_i] + shared_sum[local_i + stride]\n","            )\n","        barrier()\n","        stride = stride // 2\n","\n","    block_sum = shared_sum[0]\n","\n","    # Normalize by sum\n","    if global_i < seq_len:\n","        output[global_i] = output[global_i] / block_sum\n","\n","\n","# CPU implementation for vector attention\n","fn attention_cpu_kernel[\n","    layout_q: Layout,\n","    layout_k: Layout,\n","    layout_v: Layout,\n","    layout_out: Layout,\n","    seq_len: Int,\n","    d: Int,\n","    dtype: DType = DType.float32,\n","](\n","    output: LayoutTensor[dtype, layout_out, MutableAnyOrigin],\n","    q: LayoutTensor[dtype, layout_q, MutableAnyOrigin],\n","    k: LayoutTensor[dtype, layout_k, MutableAnyOrigin],\n","    v: LayoutTensor[dtype, layout_v, MutableAnyOrigin],\n","):\n","    var scores = List[Float32]()\n","    var weights = List[Float32]()\n","    for _ in range(seq_len):\n","        scores.append(0.0)\n","        weights.append(0.0)\n","\n","    # CPU: Compute attention scores K[i] · Q directly for each row i of K\n","    for i in range(seq_len):\n","        var score: Float32 = 0.0\n","        for dim in range(d):\n","            score = score + rebind[Float32](q[dim]) * rebind[Float32](k[i, dim])\n","        scores[i] = score\n","\n","    var max_score: Float32 = scores[0]\n","    for i in range(1, seq_len):\n","        if scores[i] > max_score:\n","            max_score = scores[i]\n","\n","    var sum_exp: Float32 = 0.0\n","    for i in range(seq_len):\n","        weights[i] = exp(scores[i] - max_score)\n","        sum_exp = sum_exp + weights[i]\n","\n","    for i in range(seq_len):\n","        weights[i] = weights[i] / sum_exp\n","\n","    for dim in range(d):\n","        var weighted_sum: Float32 = 0.0\n","        for i in range(seq_len):\n","            weighted_sum = weighted_sum + weights[i] * rebind[Float32](\n","                v[i, dim]\n","            )\n","        output[dim] = rebind[Scalar[dtype]](weighted_sum)\n","\n","\n","@compiler.register(\"attention\")\n","struct AttentionCustomOp:\n","    @staticmethod\n","    fn execute[\n","        target: StaticString,  # \"cpu\" or \"gpu\"\n","        seq_len: Int,\n","        d: Int,\n","        dtype: DType = DType.float32,\n","    ](\n","        output: OutputTensor[rank=1],  # Output vector (d,)\n","        q: InputTensor[rank=1],  # Query vector (d,)\n","        k: InputTensor[rank=2],  # Key matrix (seq_len, d)\n","        v: InputTensor[rank=2],  # Value matrix (seq_len, d)\n","        ctx: DeviceContextPtr,\n","    ) raises:\n","        # Define layouts\n","        alias layout_q = Layout.row_major(d)\n","        alias layout_k = Layout.row_major(seq_len, d)\n","        alias layout_v = Layout.row_major(seq_len, d)\n","        alias layout_out = Layout.row_major(d)\n","        alias layout_scores = Layout.row_major(seq_len)\n","\n","        # Convert to layout tensors\n","        var output_tensor = rebind[\n","            LayoutTensor[dtype, layout_out, MutableAnyOrigin]\n","        ](output.to_layout_tensor())\n","        var q_tensor = rebind[LayoutTensor[dtype, layout_q, MutableAnyOrigin]](\n","            q.to_layout_tensor()\n","        )\n","        var k_tensor = rebind[LayoutTensor[dtype, layout_k, MutableAnyOrigin]](\n","            k.to_layout_tensor()\n","        )\n","        var v_tensor = rebind[LayoutTensor[dtype, layout_v, MutableAnyOrigin]](\n","            v.to_layout_tensor()\n","        )\n","\n","        @parameter\n","        if target == \"gpu\":\n","            # ANCHOR: attention_orchestration\n","            var gpu_ctx = rebind[DeviceContext](ctx[])\n","\n","            # Define layouts for matrix multiplication\n","            # Q reshaped to (1, d)\n","            alias layout_q_2d = Layout.row_major(1, d)\n","            # K^T is (d, seq_len)\n","            alias layout_k_t = Layout.row_major(d, seq_len)\n","            # Scores as (1, seq_len)\n","            alias layout_scores_2d = Layout.row_major(1, seq_len)\n","            # Weights as (1, seq_len)\n","            alias layout_weights_2d = Layout.row_major(1, seq_len)\n","            # Result as (1, d)\n","            alias layout_result_2d = Layout.row_major(1, d)\n","\n","            alias scores_blocks_per_grid = (\n","                (seq_len + TPB - 1) // TPB,\n","                (1 + TPB - 1) // TPB,\n","            )\n","            alias result_blocks_per_grid = (\n","                (d + TPB - 1) // TPB,\n","                (1 + TPB - 1) // TPB,\n","            )\n","            alias matmul_threads_per_block = (TPB, TPB)\n","            alias transpose_blocks_per_grid = (\n","                (seq_len + TPB - 1) // TPB,\n","                (d + TPB - 1) // TPB,\n","            )\n","\n","            # Allocate minimal temporary buffers - reuse same buffer for different shapes\n","            k_t_buf = gpu_ctx.enqueue_create_buffer[dtype](\n","                seq_len * d\n","            )  # K^T as (d, seq_len)\n","            scores_weights_buf = gpu_ctx.enqueue_create_buffer[dtype](\n","                seq_len\n","            )  # Reused for scores and weights\n","\n","            k_t = LayoutTensor[mut=True, dtype, layout_k_t, MutableAnyOrigin](\n","                k_t_buf.unsafe_ptr()\n","            )\n","\n","            # Step 1: Reshape Q from (d,) to (1, d) - no buffer needed\n","            # FILL ME IN 1 line\n","            q_2d = q_tensor.reshape[layout_q_2d]()\n","\n","            # Step 2: Transpose K from (seq_len, d) to K^T (d, seq_len)\n","            # FILL ME IN 1 function call\n","            gpu_ctx.enqueue_function[\n","              transpose_kernel[layout_k, layout_k_t, seq_len, d, dtype]\n","            ](\n","              k_t,\n","              k_tensor,\n","              grid_dim=transpose_blocks_per_grid,\n","              block_dim=matmul_threads_per_block,\n","            )\n","\n","            # Step 3: Compute attention scores using matmul: Q @ K^T = (1, d) @ (d, seq_len) -> (1, seq_len)\n","            # GPU: Uses matrix multiplication to compute all Q · K[i] scores in parallel\n","            # Reuse scores_weights_buf as (1, seq_len) for scores\n","            # FILL ME IN 2 lines\n","            scores_2d = LayoutTensor[\n","              mut=True, dtype, layout_scores_2d, MutableAnyOrigin\n","            ](scores_weights_buf.unsafe_ptr())\n","\n","            gpu_ctx.enqueue_function[\n","              matmul_idiomatic_tiled[layout_q_2d, 1, seq_len, d, dtype]\n","            ](\n","              scores_2d,\n","              q_2d,\n","              k_t,\n","              grid_dim=scores_blocks_per_grid,\n","              block_dim=matmul_threads_per_block,\n","            )\n","\n","            # Step 4: Reshape scores from (1, seq_len) to (seq_len,) for softmax\n","            # FILL ME IN 1 line\n","            weights = scores_2d.reshape[layout_scores]()\n","\n","            # Step 5: Apply softmax to get attention weights\n","            # FILL ME IN 1 function call\n","            gpu_ctx.enqueue_function[\n","              softmax_gpu_kernel[layout_scores, seq_len, dtype]\n","            ](\n","              weights,\n","              weights,\n","              grid_dim=(1, 1),\n","              block_dim=(seq_len, 1),\n","            )\n","\n","            # Step 6: Reshape weights from (seq_len,) to (1, seq_len) for final matmul\n","            # FILL ME IN 1 line\n","            weights_2d = weights.reshape[layout_weights_2d]()\n","\n","            # Step 7: Compute final result using matmul: weights @ V = (1, seq_len) @ (seq_len, d) -> (1, d)\n","            # Reuse out_tensor reshaped as (1, d) for result\n","            # FILL ME IN 2 lines\n","            result_2d = output_tensor.reshape[layout_result_2d]()\n","            gpu_ctx.enqueue_function[\n","              matmul_idiomatic_tiled[layout_weights_2d, 1, d, seq_len, dtype]\n","            ](\n","              result_2d,\n","              weights_2d,\n","              v_tensor,\n","              grid_dim=result_blocks_per_grid,\n","              block_dim=matmul_threads_per_block,\n","            )\n","\n","            # ANCHOR_END: attention_orchestration\n","\n","        elif target == \"cpu\":\n","            attention_cpu_kernel[\n","                layout_q, layout_k, layout_v, layout_out, seq_len, d, dtype\n","            ](output_tensor, q_tensor, k_tensor, v_tensor)\n","\n","        else:\n","            raise Error(\"Unsupported target: \" + target)\n","\n","\"\"\""],"metadata":{"id":"OxediTSU1H9b","executionInfo":{"status":"ok","timestamp":1756557722974,"user_tz":-330,"elapsed":215,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p19/op/attention.mojo\")"],"metadata":{"id":"XALncN2W1M1g","executionInfo":{"status":"ok","timestamp":1756557722991,"user_tz":-330,"elapsed":10,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!cd /content/mojo-gpu-puzzles && uv run poe p19"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4oZPw7W1M3r","executionInfo":{"status":"ok","timestamp":1756557947177,"user_tz":-330,"elapsed":16678,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"09eb416e-faca-4734-b6a5-517fe7b2e520"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo package problems/p19/op -o problems/p19/op.mojopkg\u001b[0m\n","\u001b[37mPoe =>\u001b[0m \u001b[94mpython problems/p19/p19.py\u001b[0m\n","Input shapes: Q=(16,), K=(16, 16), V=(16, 16)\n","Sample Q values: [ 0.04967142 -0.01382643  0.06476886  0.15230298 -0.02341534]\n","Sample K[0] values: [-0.10128311  0.03142473 -0.09080241 -0.14123037  0.14656489]\n","Sample V[0] values: [ 0.11631638  0.00102331 -0.09815087  0.04621035  0.01990597]\n","\n","================================================================================\n","STEP-BY-STEP VECTOR ATTENTION COMPUTATION DEBUG\n","================================================================================\n","\n","1. INPUT SHAPES:\n","   Q shape: (16,) (query vector)\n","   K shape: (16, 16) (key matrix)\n","   V shape: (16, 16) (value matrix)\n","   Q[:5]: [ 0.04967142 -0.01382643  0.06476886  0.15230298 -0.02341534]\n","\n","2. ATTENTION SCORES (K[i] · Q):\n","   Scores shape: (16,)\n","   Scores[:5]: [-0.03479404 -0.01563787  0.04834607  0.06764711  0.04001468]\n","   Min: -0.061636, Max: 0.067647\n","   Manual verification:\n","     Q · K[0] = K[0] · Q = -0.034794 (computed: -0.034794)\n","     Q · K[1] = K[1] · Q = -0.015638 (computed: -0.015638)\n","     Q · K[2] = K[2] · Q = 0.048346 (computed: 0.048346)\n","\n","3. SOFTMAX:\n","   Max score: 0.067647\n","   Attention weights shape: (16,)\n","   Attention weights[:5]: [0.05981331 0.06097015 0.06499878 0.0662655  0.06445949]\n","   Sum: 1.000000 (should be 1.0)\n","\n","4. WEIGHTED SUM OF VALUES:\n","   Output shape: (16,)\n","   Output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]\n","   Output norm: 0.092764\n","   Manual output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]\n","   Match: True\n","\n","================================================================================\n","TESTING INDIVIDUAL OPERATIONS\n","================================================================================\n","\n","Test 1: Vector Dot Product\n","a · b = 3.000000\n","\n","Test 2: Matrix-Vector Multiplication\n","M @ v = [ 3.  7. 11.]\n","\n","Test 3: Softmax\n","Input: [1. 2. 3. 4.]\n","Softmax: [0.0320586  0.08714432 0.2368828  0.6439143 ]\n","Sum: 1.000000\n","\n","================================================================================\n","TESTING FULL ATTENTION\n","================================================================================\n","Compiling attention graph on Device(type=cpu,id=0)\n","Executing attention on Device(type=cpu,id=0)\n","====================================================================================================\n","\n","CPU attention output[:5]: [-0.00935538 -0.02434331  0.00306551  0.02346884  0.019306  ]\n","CPU matches NumPy: True\n","Compiling attention graph on Device(type=gpu,id=0)\n","LLVM ERROR: Cannot select: intrinsic %llvm.nvvm.cp.async.wait.all\n","\u001b[91;1mError: Sequence aborted after failed subtask 'p19[1]'\u001b[0m\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Kf1TqSJb1M5h","executionInfo":{"status":"ok","timestamp":1756557862462,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":8,"outputs":[]}]}