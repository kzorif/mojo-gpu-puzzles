{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1huglcIpa-mveWZ5I0llqHAI8-l-TBv_7","timestamp":1756950224572}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJYqBcBV4akx","executionInfo":{"status":"ok","timestamp":1756705135691,"user_tz":-330,"elapsed":28644,"user":{"displayName":"firoz","userId":"03277806218454595224"}},"outputId":"550cae52-cfe8-48ad-ab72-dfa068af7536"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max==25.4.0\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":["!pip install max==25.4.0 --index-url https://dl.modular.com/public/nightly/python/simple/"]},{"cell_type":"code","source":["!git clone https://github.com/modular/mojo-gpu-puzzles"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGNGs6Z14hoy","executionInfo":{"status":"ok","timestamp":1756705142876,"user_tz":-330,"elapsed":7183,"user":{"displayName":"firoz","userId":"03277806218454595224"}},"outputId":"0c8e136a-ffd2-4ff4-f712-b0cc52e7ecfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 6332, done.\u001b[K\n","remote: Counting objects: 100% (481/481), done.\u001b[K\n","remote: Compressing objects: 100% (65/65), done.\u001b[K\n","remote: Total 6332 (delta 449), reused 416 (delta 416), pack-reused 5851 (from 3)\u001b[K\n","Receiving objects: 100% (6332/6332), 148.64 MiB | 32.18 MiB/s, done.\n","Resolving deltas: 100% (3923/3923), done.\n"]}]},{"cell_type":"code","source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJ7vri8B4l--","executionInfo":{"status":"ok","timestamp":1756705144597,"user_tz":-330,"elapsed":1712,"user":{"displayName":"firoz","userId":"03277806218454595224"}},"outputId":"87bad6c7-e332-4e6d-a8af-924c8c4fd6d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.14 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}]},{"cell_type":"code","source":["import max.support.notebook"],"metadata":{"id":"Z1C5WQhB4sZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"],"metadata":{"id":"VTgatNSu4sdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mojo_code = \"\"\"\n","from math import ceildiv\n","from gpu import thread_idx, block_idx, block_dim, barrier, lane_id\n","from gpu.host import DeviceContext\n","from gpu.warp import sum as warp_sum, WARP_SIZE\n","from algorithm.functional import elementwise\n","from layout import Layout, LayoutTensor\n","from layout.tensor_builder import LayoutTensorBuild as tb\n","from utils import IndexList\n","from sys.info import simdwidthof, sizeof, alignof\n","from sys.arg import argv\n","from testing import assert_equal\n","from benchmark import (\n","    Bench,\n","    BenchConfig,\n","    Bencher,\n","    BenchId,\n","    keep,\n","    ThroughputMeasure,\n","    BenchMetric,\n","    BenchmarkInfo,\n","    run,\n",")\n","\n","# ANCHOR: traditional_approach_from_p12\n","alias SIZE = WARP_SIZE\n","alias BLOCKS_PER_GRID = (1, 1)\n","alias THREADS_PER_BLOCK = (WARP_SIZE, 1)  # optimal choice for warp kernel\n","alias dtype = DType.float32\n","alias SIMD_WIDTH = simdwidthof[Float32]()\n","alias in_layout = Layout.row_major(SIZE)\n","alias out_layout = Layout.row_major(1)\n","\n","\n","fn traditional_dot_product_p12_style[\n","    in_layout: Layout, out_layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=True, dtype, out_layout],\n","    a: LayoutTensor[mut=False, dtype, in_layout],\n","    b: LayoutTensor[mut=False, dtype, in_layout],\n","):\n","    shared = tb[dtype]().row_major[WARP_SIZE]().shared().alloc()\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","\n","    if global_i < size:\n","        shared[local_i] = (a[global_i] * b[global_i]).reduce_add()\n","    else:\n","        shared[local_i] = 0.0\n","\n","    barrier()\n","\n","    stride = SIZE // 2\n","    while stride > 0:\n","        if local_i < stride:\n","            shared[local_i] += shared[local_i + stride]\n","        barrier()\n","        stride //= 2\n","\n","    if local_i == 0:\n","        output[0] = shared[0]\n","\n","\n","# ANCHOR_END: traditional_approach_from_p12\n","\n","# ANCHOR: simple_warp_kernel\n","from gpu.warp import sum as warp_sum\n","\n","\n","fn simple_warp_dot_product[\n","    in_layout: Layout, out_layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=True, dtype, out_layout],\n","    a: LayoutTensor[mut=False, dtype, in_layout],\n","    b: LayoutTensor[mut=False, dtype, in_layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    # FILL IN (6 lines at most)\n","    var partial_product: Scalar[dtype] = 0\n","    if global_i < size:\n","        partial_product = (a[global_i] * b[global_i]).reduce_add()\n","\n","    total = warp_sum(partial_product)\n","    if lane_id() == 0:\n","        output[0] = total\n","\n","# ANCHOR_END: simple_warp_kernel\n","\n","# ANCHOR: functional_warp_approach\n","fn functional_warp_dot_product[\n","    layout: Layout, dtype: DType, simd_width: Int, rank: Int, size: Int\n","](\n","    output: LayoutTensor[\n","        mut=True, dtype, Layout.row_major(1), MutableAnyOrigin\n","    ],\n","    a: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","    b: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],\n","    ctx: DeviceContext,\n",") raises:\n","    @parameter\n","    @always_inline\n","    fn compute_dot_product[\n","        simd_width: Int, rank: Int, alignment: Int = alignof[dtype]()\n","    ](indices: IndexList[rank]) capturing -> None:\n","        idx = indices[0]\n","        print(\"idx:\", idx)\n","        # FILL IN (10 lines at most)\n","        var partial_product: Scalar[dtype] = 0.0\n","        if idx < size:\n","            a_val = a.load[1](idx, 0)\n","            b_val = b.load[1](idx, 0)\n","            partial_product = (a_val * b_val).reduce_add()\n","        else:\n","            partial_product = 0.0\n","\n","        total = warp_sum(partial_product)\n","\n","        if lane_id() == 0:\n","            output.store[1](0, 0, total)\n","\n","    # Launch exactly WARP_SIZE threads (one warp) to process all elements\n","    elementwise[compute_dot_product, 1, target=\"gpu\"](WARP_SIZE, ctx)\n","\n","\n","# ANCHOR_END: functional_warp_approach\n","\n","\n","@parameter\n","@always_inline\n","fn benchmark_simple_warp_parameterized[test_size: Int](mut b: Bencher) raises:\n","    @parameter\n","    @always_inline\n","    fn simple_warp_workflow(ctx: DeviceContext) raises:\n","        alias test_layout = Layout.row_major(test_size)\n","        alias test_blocks = (ceildiv(test_size, WARP_SIZE), 1)\n","\n","        out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","        a = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","        b_buf = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","\n","        with a.map_to_host() as a_host, b_buf.map_to_host() as b_host:\n","            for i in range(test_size):\n","                a_host[i] = i\n","                b_host[i] = i\n","\n","        out_tensor = LayoutTensor[dtype, out_layout](out.unsafe_ptr())\n","        a_tensor = LayoutTensor[dtype, test_layout](a.unsafe_ptr())\n","        b_tensor = LayoutTensor[dtype, test_layout](b_buf.unsafe_ptr())\n","\n","        ctx.enqueue_function[\n","            simple_warp_dot_product[test_layout, out_layout, test_size]\n","        ](\n","            out_tensor,\n","            a_tensor,\n","            b_tensor,\n","            grid_dim=test_blocks,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","        keep(out.unsafe_ptr())\n","        keep(a.unsafe_ptr())\n","        keep(b_buf.unsafe_ptr())\n","        ctx.synchronize()\n","\n","    bench_ctx = DeviceContext()\n","    b.iter_custom[simple_warp_workflow](bench_ctx)\n","\n","\n","@parameter\n","@always_inline\n","fn benchmark_functional_warp_parameterized[\n","    test_size: Int\n","](mut b: Bencher) raises:\n","    @parameter\n","    @always_inline\n","    fn functional_warp_workflow(ctx: DeviceContext) raises:\n","        alias test_layout = Layout.row_major(test_size)\n","\n","        out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","        a = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","        b_buf = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","\n","        with a.map_to_host() as a_host, b_buf.map_to_host() as b_host:\n","            for i in range(test_size):\n","                a_host[i] = i\n","                b_host[i] = i\n","\n","        a_tensor = LayoutTensor[mut=False, dtype, test_layout](a.unsafe_ptr())\n","        b_tensor = LayoutTensor[mut=False, dtype, test_layout](\n","            b_buf.unsafe_ptr()\n","        )\n","        out_tensor = LayoutTensor[mut=True, dtype, Layout.row_major(1)](\n","            out.unsafe_ptr()\n","        )\n","\n","        functional_warp_dot_product[\n","            test_layout, dtype, SIMD_WIDTH, 1, test_size\n","        ](out_tensor, a_tensor, b_tensor, ctx)\n","        keep(out.unsafe_ptr())\n","        keep(a.unsafe_ptr())\n","        keep(b_buf.unsafe_ptr())\n","        ctx.synchronize()\n","\n","    bench_ctx = DeviceContext()\n","    b.iter_custom[functional_warp_workflow](bench_ctx)\n","\n","\n","@parameter\n","@always_inline\n","fn benchmark_traditional_parameterized[test_size: Int](mut b: Bencher) raises:\n","    @parameter\n","    @always_inline\n","    fn traditional_workflow(ctx: DeviceContext) raises:\n","        alias test_layout = Layout.row_major(test_size)\n","        alias test_blocks = (ceildiv(test_size, WARP_SIZE), 1)\n","\n","        out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","        a = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","        b_buf = ctx.enqueue_create_buffer[dtype](test_size).enqueue_fill(0)\n","\n","        with a.map_to_host() as a_host, b_buf.map_to_host() as b_host:\n","            for i in range(test_size):\n","                a_host[i] = i\n","                b_host[i] = i\n","\n","        out_tensor = LayoutTensor[dtype, out_layout](out.unsafe_ptr())\n","        a_tensor = LayoutTensor[dtype, test_layout](a.unsafe_ptr())\n","        b_tensor = LayoutTensor[dtype, test_layout](b_buf.unsafe_ptr())\n","\n","        ctx.enqueue_function[\n","            traditional_dot_product_p12_style[\n","                test_layout, out_layout, test_size\n","            ]\n","        ](\n","            out_tensor,\n","            a_tensor,\n","            b_tensor,\n","            grid_dim=test_blocks,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","        keep(out.unsafe_ptr())\n","        keep(a.unsafe_ptr())\n","        keep(b_buf.unsafe_ptr())\n","        ctx.synchronize()\n","\n","    bench_ctx = DeviceContext()\n","    b.iter_custom[traditional_workflow](bench_ctx)\n","\n","\n","def main():\n","    with DeviceContext() as ctx:\n","        out = ctx.enqueue_create_buffer[dtype](1).enqueue_fill(0)\n","        a = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","        b = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","        with a.map_to_host() as a_host, b.map_to_host() as b_host:\n","            for i in range(SIZE):\n","                a_host[i] = i\n","                b_host[i] = i\n","\n","        out_tensor = LayoutTensor[mut=True, dtype, out_layout](out.unsafe_ptr())\n","        a_tensor = LayoutTensor[mut=False, dtype, in_layout](a.unsafe_ptr())\n","        b_tensor = LayoutTensor[mut=False, dtype, in_layout](b.unsafe_ptr())\n","\n","        print(\"SIZE:\", SIZE)\n","        print(\"WARP_SIZE:\", WARP_SIZE)\n","        print(\"SIMD_WIDTH:\", SIMD_WIDTH)\n","        if argv()[1] == \"--traditional\":\n","            ctx.enqueue_function[\n","                traditional_dot_product_p12_style[in_layout, out_layout, SIZE]\n","            ](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                grid_dim=BLOCKS_PER_GRID,\n","                block_dim=THREADS_PER_BLOCK,\n","            )\n","        elif argv()[1] == \"--kernel\":\n","            ctx.enqueue_function[\n","                simple_warp_dot_product[in_layout, out_layout, SIZE]\n","            ](\n","                out_tensor,\n","                a_tensor,\n","                b_tensor,\n","                grid_dim=BLOCKS_PER_GRID,\n","                block_dim=THREADS_PER_BLOCK,\n","            )\n","\n","        elif argv()[1] == \"--functional\":\n","            functional_warp_dot_product[in_layout, dtype, SIMD_WIDTH, 1, SIZE](\n","                out_tensor, a_tensor, b_tensor, ctx\n","            )\n","\n","        elif argv()[1] == \"--benchmark\":\n","            print(\"-\" * 80)\n","            bench_config = BenchConfig(max_iters=100)\n","            bench = Bench(bench_config)\n","\n","            print(\"Testing SIZE=1 x WARP_SIZE, BLOCKS=1\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[WARP_SIZE]\n","            ](BenchId(\"traditional_1x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[WARP_SIZE]\n","            ](BenchId(\"simple_warp_1x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[WARP_SIZE]\n","            ](BenchId(\"functional_warp_1x\"))\n","\n","            print(\"-\" * 80)\n","            print(\"Testing SIZE=4 x WARP_SIZE, BLOCKS=4\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[4 * WARP_SIZE]\n","            ](BenchId(\"traditional_4x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[4 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_4x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[4 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_4x\"))\n","\n","            print(\"-\" * 80)\n","            print(\"Testing SIZE=32 x WARP_SIZE, BLOCKS=32\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[32 * WARP_SIZE]\n","            ](BenchId(\"traditional_32x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[32 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_32x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[32 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_32x\"))\n","\n","            print(\"-\" * 80)\n","            print(\"Testing SIZE=256 x WARP_SIZE, BLOCKS=256\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[256 * WARP_SIZE]\n","            ](BenchId(\"traditional_256x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[256 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_256x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[256 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_256x\"))\n","\n","            print(\"-\" * 80)\n","            print(\"Testing SIZE=2048 x WARP_SIZE, BLOCKS=2048\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[2048 * WARP_SIZE]\n","            ](BenchId(\"traditional_2048x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[2048 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_2048x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[2048 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_2048x\"))\n","\n","            print(\"-\" * 80)\n","            print(\"Testing SIZE=16384 x WARP_SIZE, BLOCKS=16384 (Large Scale)\")\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[16384 * WARP_SIZE]\n","            ](BenchId(\"traditional_16384x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[16384 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_16384x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[16384 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_16384x\"))\n","\n","            print(\"-\" * 80)\n","            print(\n","                \"Testing SIZE=65536 x WARP_SIZE, BLOCKS=65536 (Massive Scale)\"\n","            )\n","            bench.bench_function[\n","                benchmark_traditional_parameterized[65536 * WARP_SIZE]\n","            ](BenchId(\"traditional_65536x\"))\n","            bench.bench_function[\n","                benchmark_simple_warp_parameterized[65536 * WARP_SIZE]\n","            ](BenchId(\"simple_warp_65536x\"))\n","            bench.bench_function[\n","                benchmark_functional_warp_parameterized[65536 * WARP_SIZE]\n","            ](BenchId(\"functional_warp_65536x\"))\n","\n","            print(bench)\n","            print(\"Benchmarks completed!\")\n","            print()\n","            print(\"ðŸš€ WARP OPERATIONS PERFORMANCE ANALYSIS:\")\n","            print(\n","                \"   GPU Architecture: NVIDIA (WARP_SIZE=32) vs AMD\"\n","                \" (WARP_SIZE=64)\"\n","            )\n","            print(\"   - 1 x WARP_SIZE: Single warp baseline\")\n","            print(\"   - 4 x WARP_SIZE: Few warps, warp overhead visible\")\n","            print(\"   - 32 x WARP_SIZE: Medium scale, warp benefits emerge\")\n","            print(\"   - 256 x WARP_SIZE: Large scale, dramatic warp advantages\")\n","            print(\n","                \"   - 2048 x WARP_SIZE: Massive scale, warp operations dominate\"\n","            )\n","            print(\"   - 16384 x WARP_SIZE: Large scale (512K-1M elements)\")\n","            print(\"   - 65536 x WARP_SIZE: Massive scale (2M-4M elements)\")\n","            print(\n","                \"   - Note: AMD GPUs process 2 x elements per warp vs NVIDIA!\"\n","            )\n","            print()\n","            print(\"   Expected Results at Large Scales:\")\n","            print(\"   â€¢ Traditional: Slower due to more barrier overhead\")\n","            print(\n","                \"   â€¢ Warp operations: Faster, scale better with problem size\"\n","            )\n","            print(\"   â€¢ Memory bandwidth becomes the limiting factor\")\n","            return\n","\n","        else:\n","            print(\n","                \"Usage: --traditional | --kernel | --functional | --benchmark\"\n","            )\n","            return\n","\n","        expected = ctx.enqueue_create_host_buffer[dtype](1).enqueue_fill(0)\n","        ctx.synchronize()\n","\n","        with a.map_to_host() as a_host, b.map_to_host() as b_host:\n","            for i in range(SIZE):\n","                expected[0] += a_host[i] * b_host[i]\n","\n","        with out.map_to_host() as out_host:\n","            print(\"=== RESULT ===\")\n","            print(\"out:\", out_host[0])\n","            print(\"expected:\", expected[0])\n","            assert_equal(out_host[0], expected[0])\n","\n","        if len(argv()) == 1 or argv()[1] == \"--kernel\":\n","            print()\n","            print(\n","                \"ðŸš€ Notice how simple the warp version is compared to p10.mojo!\"\n","            )\n","            print(\n","                \"   Same kernel structure, but warp_sum() replaces all the\"\n","                \" complexity!\"\n","            )\n","        elif argv()[1] == \"--functional\":\n","            print()\n","            print(\n","                \"ðŸ”§ Functional approach shows modern Mojo style with warp\"\n","                \" operations!\"\n","            )\n","            print(\n","                \"   Clean, composable, and still leverages warp hardware\"\n","                \" primitives!\"\n","            )\n","\"\"\""],"metadata":{"id":"uvmr-qmO4sfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p24/p24.mojo\")"],"metadata":{"id":"ylU4ZF5z4shT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cd /content/mojo-gpu-puzzles && uv run poe p24 --functional"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQ7ZwDWj49vU","executionInfo":{"status":"ok","timestamp":1756705304871,"user_tz":-330,"elapsed":32029,"user":{"displayName":"firoz","userId":"03277806218454595224"}},"outputId":"4cac1c5d-f4b1-4514-de83-8cfdb3605ce3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo problems/p24/p24.mojo --functional\u001b[0m\n","\u001b[1m/content/mojo-gpu-puzzles/problems/p24/p24.mojo:30:40: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mUse `sys.simd_width_of()` instead.\n","\u001b[0malias SIMD_WIDTH = simdwidthof[Float32]()\n","\u001b[0;1;32m                   ~~~~~~~~~~~~~~~~~~~~^~\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p24/p24.mojo:1:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m\u001b[1m'simdwidthof' declared here\n","\u001b[0m\n","\u001b[0;1;32m^\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p24/p24.mojo:103:68: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mUse `sys.align_of()` instead.\n","\u001b[0m        simd_width: Int, rank: Int, alignment: Int = alignof[dtype]()\n","\u001b[0;1;32m                                                     ~~~~~~~~~~~~~~^~\n","\u001b[0m\u001b[1m/content/mojo-gpu-puzzles/problems/p24/p24.mojo:1:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m\u001b[1m'alignof' declared here\n","\u001b[0m\n","\u001b[0;1;32m^\n","\u001b[0mSIZE: 32\n","WARP_SIZE: 32\n","SIMD_WIDTH: 8\n","idx: 0\n","idx: 1\n","idx: 2\n","idx: 3\n","idx: 4\n","idx: 5\n","idx: 6\n","idx: 7\n","idx: 8\n","idx: 9\n","idx: 10\n","idx: 11\n","idx: 12\n","idx: 13\n","idx: 14\n","idx: 15\n","idx: 16\n","idx: 17\n","idx: 18\n","idx: 19\n","idx: 20\n","idx: 21\n","idx: 22\n","idx: 23\n","idx: 24\n","idx: 25\n","idx: 26\n","idx: 27\n","idx: 28\n","idx: 29\n","idx: 30\n","idx: 31\n","=== RESULT ===\n","out: 10416.0\n","expected: 10416.0\n","\n","ðŸ”§ Functional approach shows modern Mojo style with warp operations!\n","   Clean, composable, and still leverages warp hardware primitives!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"baVd9LFI49xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jAX7P6Mt49zh"},"execution_count":null,"outputs":[]}]}