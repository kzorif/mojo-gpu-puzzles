{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPA/G5ruKrwpw2/f0aU/HRr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CujEhsIHEyR","executionInfo":{"status":"ok","timestamp":1756753059373,"user_tz":-330,"elapsed":28635,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"85990911-057d-409f-d50a-16e05c1b41bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max==25.4.0\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max==25.4.0) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":[]},{"cell_type":"code","source":["!git clone https://github.com/modular/mojo-gpu-puzzles"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFGYrMpsHp1m","executionInfo":{"status":"ok","timestamp":1756753067591,"user_tz":-330,"elapsed":8215,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"e892e921-fa46-4e2f-da22-db72bbc0ed02"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 6332, done.\u001b[K\n","remote: Counting objects: 100% (481/481), done.\u001b[K\n","remote: Compressing objects: 100% (65/65), done.\u001b[K\n","remote: Total 6332 (delta 449), reused 416 (delta 416), pack-reused 5851 (from 3)\u001b[K\n","Receiving objects: 100% (6332/6332), 148.64 MiB | 26.30 MiB/s, done.\n","Resolving deltas: 100% (3923/3923), done.\n"]}]},{"cell_type":"code","source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"euWZUyUgHq-1","executionInfo":{"status":"ok","timestamp":1756753069247,"user_tz":-330,"elapsed":1654,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"4faeed9d-d3c9-492b-a8d5-e78bc26027b2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.14 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}]},{"cell_type":"code","source":["import max.support.notebook"],"metadata":{"id":"FoHDo71RHsIh","executionInfo":{"status":"ok","timestamp":1756753069264,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"],"metadata":{"id":"Ql1Z4Uf9HtbE","executionInfo":{"status":"ok","timestamp":1756753069267,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["mojo_code = \"\"\"\n","from gpu import thread_idx, block_idx, block_dim, lane_id\n","from gpu.host import DeviceContext\n","from gpu.warp import shuffle_down, broadcast, WARP_SIZE\n","from layout import Layout, LayoutTensor\n","from sys import argv\n","from testing import assert_equal, assert_almost_equal\n","\n","# ANCHOR: neighbor_difference\n","alias SIZE = WARP_SIZE\n","alias BLOCKS_PER_GRID = (1, 1)\n","alias THREADS_PER_BLOCK = (WARP_SIZE, 1)\n","alias dtype = DType.float32\n","alias layout = Layout.row_major(SIZE)\n","\n","\n","fn neighbor_difference[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    input: LayoutTensor[mut=False, dtype, layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    lane = lane_id()\n","\n","    # FILL IN (roughly 7 lines)\n","    if global_i < size:\n","        current_val = input[global_i]\n","        next_val = shuffle_down(current_val, 1)\n","        if lane < WARP_SIZE - 1:\n","            output[global_i] = next_val - current_val\n","        else:\n","            output[global_i] = 0\n","\n","# ANCHOR_END: neighbor_difference\n","\n","# ANCHOR: moving_average_3\n","alias SIZE_2 = 64\n","alias BLOCKS_PER_GRID_2 = (2, 1)\n","alias THREADS_PER_BLOCK_2 = (WARP_SIZE, 1)\n","alias layout_2 = Layout.row_major(SIZE_2)\n","\n","\n","fn moving_average_3[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    input: LayoutTensor[mut=False, dtype, layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    lane = lane_id()\n","\n","    # FILL IN (roughly 10 lines)\n","    if global_i < size:\n","        current_val = input[global_i]\n","        next_val = shuffle_down(current_val, 1)\n","        next_next_val = shuffle_down(current_val, 2)\n","\n","        if lane < WARP_SIZE - 1 and global_i < size - 2:\n","            output[global_i] = (current_val + next_val + next_next_val) / 3.0\n","        elif lane < WARP_SIZE - 1 and global_i < size - 1:\n","            output[global_i] = (current_val + next_val) / 2.0\n","        else:\n","            output[global_i] = current_val\n","\n","# ANCHOR_END: moving_average_3\n","\n","\n","# ANCHOR: broadcast_shuffle_coordination\n","fn broadcast_shuffle_coordination[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    input: LayoutTensor[mut=False, dtype, layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    lane = lane_id()\n","    if global_i < size:\n","        var scale_factor: output.element_type = 0.0\n","\n","        # FILL IN (roughly 14 lines)\n","        if lane == 0:\n","            block_start = block_idx.x * block_dim.x\n","            var sum: output.element_type = 0.0\n","            for i in range(4):\n","                if block_start + i < size:\n","                    sum += input[block_start + i]\n","            scale_factor = sum / 4.0\n","\n","        scale_factor = broadcast(scale_factor)\n","\n","        current_val = input[global_i]\n","        next_val = shuffle_down(current_val , 1)\n","\n","        if lane < WARP_SIZE - 1 and global_i < size - 1:\n","            output[global_i] = (current_val + next_val) * scale_factor\n","        else:\n","            output[global_i] = current_val * scale_factor\n","\n","# ANCHOR_END: broadcast_shuffle_coordination\n","\n","\n","# ANCHOR: basic_broadcast\n","fn basic_broadcast[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    input: LayoutTensor[mut=False, dtype, layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    lane = lane_id()\n","    if global_i < size:\n","        var broadcast_value: output.element_type = 0.0\n","\n","        # FILL IN (roughly 10 lines)\n","        if lane == 0:\n","            block_start = block_idx.x * block_dim.x\n","            var sum: output.element_type = 0.0\n","            for i in range(4):\n","                if block_start + i < size:\n","                    sum += input[block_start + i]\n","            broadcast_value = sum\n","\n","        broadcast_value = broadcast(broadcast_value)\n","\n","        output[global_i] = broadcast_value + input[global_i]\n","\n","# ANCHOR_END: basic_broadcast\n","\n","\n","# ANCHOR: conditional_broadcast\n","fn conditional_broadcast[\n","    layout: Layout, size: Int\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    input: LayoutTensor[mut=False, dtype, layout],\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    lane = lane_id()\n","    if global_i < size:\n","        var decision_value: output.element_type = 0.0\n","\n","        # FILL IN (roughly 10 lines)\n","        if lane == 0:\n","            block_start = block_idx.x * block_dim.x\n","            decision_value = input[block_start] if block_start < size else 0.0\n","            for i in range(1, min(8, min(WARP_SIZE, size - block_start))):\n","                if block_start + i < size:\n","                    current_val = input[block_start + i]\n","                    if current_val > decision_value:\n","                        decision_value = current_val\n","\n","        decision_value = broadcast(decision_value)\n","\n","        current_input = input[global_i]\n","        threshold = decision_value / 2.0\n","        if current_input >= threshold:\n","            output[global_i] = current_input * 2.0  # Double if >= threshold\n","        else:\n","            output[global_i] = current_input / 2.0  # Halve if < threshold\n","\n","\n","# ANCHOR_END: conditional_broadcast\n","\n","\n","def test_neighbor_difference():\n","    with DeviceContext() as ctx:\n","        # Create test data: [0, 1, 4, 9, 16, 25, ...] (squares)\n","        input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","        output_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","        with input_buf.map_to_host() as input_host:\n","            for i in range(SIZE):\n","                input_host[i] = i * i\n","\n","        input_tensor = LayoutTensor[mut=False, dtype, layout](\n","            input_buf.unsafe_ptr()\n","        )\n","        output_tensor = LayoutTensor[mut=False, dtype, layout](\n","            output_buf.unsafe_ptr()\n","        )\n","\n","        ctx.enqueue_function[neighbor_difference[layout, SIZE]](\n","            output_tensor,\n","            input_tensor,\n","            grid_dim=BLOCKS_PER_GRID,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","\n","        expected_buf = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(\n","            0\n","        )\n","        ctx.synchronize()\n","\n","        # Create expected results: differences of squares should be odd numbers\n","        for i in range(SIZE - 1):\n","            expected_buf[i] = (i + 1) * (i + 1) - i * i\n","        expected_buf[\n","            SIZE - 1\n","        ] = 0  # Last element should be 0 (no valid neighbor)\n","\n","        with output_buf.map_to_host() as output_host:\n","            print(\"output:\", output_host)\n","            print(\"expected:\", expected_buf)\n","            for i in range(SIZE):\n","                assert_equal(output_host[i], expected_buf[i])\n","\n","    print(\"✅ Basic neighbor difference test passed!\")\n","\n","\n","def test_moving_average():\n","    with DeviceContext() as ctx:\n","        # Create test data: [1, 2, 4, 7, 11, 16, 22, 29, ...]\n","        input_buf = ctx.enqueue_create_buffer[dtype](SIZE_2).enqueue_fill(0)\n","        output_buf = ctx.enqueue_create_buffer[dtype](SIZE_2).enqueue_fill(0)\n","\n","        with input_buf.map_to_host() as input_host:\n","            input_host[0] = 1\n","            for i in range(1, SIZE_2):\n","                input_host[i] = input_host[i - 1] + i + 1\n","\n","        input_tensor = LayoutTensor[mut=False, dtype, layout_2](\n","            input_buf.unsafe_ptr()\n","        )\n","        output_tensor = LayoutTensor[mut=False, dtype, layout_2](\n","            output_buf.unsafe_ptr()\n","        )\n","\n","        ctx.enqueue_function[moving_average_3[layout_2, SIZE_2]](\n","            output_tensor,\n","            input_tensor,\n","            grid_dim=BLOCKS_PER_GRID_2,\n","            block_dim=THREADS_PER_BLOCK_2,\n","        )\n","\n","        expected_buf = ctx.enqueue_create_host_buffer[dtype](\n","            SIZE_2\n","        ).enqueue_fill(0)\n","        ctx.synchronize()\n","\n","        # Create expected results\n","        with input_buf.map_to_host() as input_host:\n","            for block in range(BLOCKS_PER_GRID_2[0]):\n","                warp_start = block * WARP_SIZE\n","                warp_end = min(warp_start + WARP_SIZE, SIZE_2)\n","\n","                for i in range(warp_start, warp_end):\n","                    lane = i % WARP_SIZE\n","                    if lane < WARP_SIZE - 2 and i < SIZE_2 - 2:\n","                        # 3-point average within warp\n","                        expected_buf[i] = (\n","                            input_host[i]\n","                            + input_host[i + 1]\n","                            + input_host[i + 2]\n","                        ) / 3.0\n","                    elif lane < WARP_SIZE - 1 and i < SIZE_2 - 1:\n","                        # 2-point average\n","                        expected_buf[i] = (\n","                            input_host[i] + input_host[i + 1]\n","                        ) / 2.0\n","                    else:\n","                        # Single value\n","                        expected_buf[i] = input_host[i]\n","\n","        with output_buf.map_to_host() as output_host:\n","            print(\"output:\", output_host)\n","            print(\"expected:\", expected_buf)\n","\n","            # Verify results\n","            for i in range(SIZE_2):\n","                assert_almost_equal(output_host[i], expected_buf[i], rtol=1e-5)\n","\n","    print(\"✅ Moving average test passed!\")\n","\n","\n","def test_broadcast_shuffle_coordination():\n","    with DeviceContext() as ctx:\n","        # Create test data: [2, 4, 6, 8, 1, 3, 5, 7, ...]\n","        input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","        output_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","        with input_buf.map_to_host() as input_host:\n","            # Create pattern: [2, 4, 6, 8, 1, 3, 5, 7, ...]\n","            for i in range(SIZE):\n","                if i < 4:\n","                    input_host[i] = (i + 1) * 2\n","                else:\n","                    input_host[i] = ((i - 4) % 4) * 2 + 1\n","\n","        input_tensor = LayoutTensor[mut=False, dtype, layout](\n","            input_buf.unsafe_ptr()\n","        )\n","        output_tensor = LayoutTensor[mut=False, dtype, layout](\n","            output_buf.unsafe_ptr()\n","        )\n","\n","        ctx.enqueue_function[broadcast_shuffle_coordination[layout, SIZE]](\n","            output_tensor,\n","            input_tensor,\n","            grid_dim=BLOCKS_PER_GRID,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","\n","        expected_buf = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(\n","            0\n","        )\n","        ctx.synchronize()\n","\n","        # Create expected results\n","        with input_buf.map_to_host() as input_host:\n","            # Lane 0 computes scale_factor from first 4 elements in block: (2+4+6+8)/4 = 5.0\n","            expected_scale = Float32(5.0)\n","\n","            for i in range(SIZE):\n","                if i < SIZE - 1:\n","                    expected_buf[i] = (\n","                        input_host[i] + input_host[i + 1]\n","                    ) * expected_scale\n","                else:\n","                    expected_buf[i] = input_host[i] * expected_scale\n","\n","        with output_buf.map_to_host() as output_host:\n","            print(\"output:\", output_host)\n","            print(\"expected:\", expected_buf)\n","            # Verify results\n","            for i in range(SIZE):\n","                assert_almost_equal(output_host[i], expected_buf[i], rtol=1e-4)\n","\n","    print(\"✅ Broadcast + Shuffle coordination test passed!\")\n","\n","\n","def test_basic_broadcast():\n","    with DeviceContext() as ctx:\n","        # Create test data: [1, 2, 3, 4, 5, 6, 7, 8, ...]\n","        input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","        output_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","        with input_buf.map_to_host() as input_host:\n","            for i in range(SIZE):\n","                input_host[i] = i + 1\n","\n","        input_tensor = LayoutTensor[mut=False, dtype, layout](\n","            input_buf.unsafe_ptr()\n","        )\n","        output_tensor = LayoutTensor[mut=False, dtype, layout](\n","            output_buf.unsafe_ptr()\n","        )\n","\n","        ctx.enqueue_function[basic_broadcast[layout, SIZE]](\n","            output_tensor,\n","            input_tensor,\n","            grid_dim=BLOCKS_PER_GRID,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","\n","        expected_buf = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(\n","            0\n","        )\n","        ctx.synchronize()\n","\n","        # Create expected results\n","        with input_buf.map_to_host() as input_host:\n","            # Lane 0 computes broadcast_value from first 4 elements: 1+2+3+4 = 10\n","            expected_broadcast = Float32(10.0)\n","            for i in range(SIZE):\n","                expected_buf[i] = expected_broadcast + input_host[i]\n","\n","        with output_buf.map_to_host() as output_host:\n","            print(\"output:\", output_host)\n","            print(\"expected:\", expected_buf)\n","\n","            # Verify results\n","            for i in range(SIZE):\n","                assert_almost_equal(output_host[i], expected_buf[i], rtol=1e-4)\n","\n","    print(\"✅ Basic broadcast test passed!\")\n","\n","\n","def test_conditional_broadcast():\n","    with DeviceContext() as ctx:\n","        # Create test data: [3, 1, 7, 2, 9, 4, 6, 8, ...]\n","        input_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","        output_buf = ctx.enqueue_create_buffer[dtype](SIZE).enqueue_fill(0)\n","\n","        with input_buf.map_to_host() as input_host:\n","            # Create pattern with known max\n","            test_values = [\n","                Float32(3.0),\n","                Float32(1.0),\n","                Float32(7.0),\n","                Float32(2.0),\n","                Float32(9.0),\n","                Float32(4.0),\n","                Float32(6.0),\n","                Float32(8.0),\n","            ]\n","            for i in range(SIZE):\n","                input_host[i] = test_values[i % len(test_values)]\n","\n","        input_tensor = LayoutTensor[mut=False, dtype, layout](\n","            input_buf.unsafe_ptr()\n","        )\n","        output_tensor = LayoutTensor[mut=False, dtype, layout](\n","            output_buf.unsafe_ptr()\n","        )\n","\n","        ctx.enqueue_function[conditional_broadcast[layout, SIZE]](\n","            output_tensor,\n","            input_tensor,\n","            grid_dim=BLOCKS_PER_GRID,\n","            block_dim=THREADS_PER_BLOCK,\n","        )\n","\n","        expected_buf = ctx.enqueue_create_host_buffer[dtype](SIZE).enqueue_fill(\n","            0\n","        )\n","        ctx.synchronize()\n","\n","        # Create expected results\n","        with input_buf.map_to_host() as input_host:\n","            # Lane 0 finds max of first 8 elements in block: max(3,1,7,2,9,4,6,8) = 9.0, threshold = 4.5\n","            expected_max = Float32(9.0)\n","            threshold = expected_max / 2.0\n","            for i in range(SIZE):\n","                if input_host[i] >= threshold:\n","                    expected_buf[i] = input_host[i] * 2.0\n","                else:\n","                    expected_buf[i] = input_host[i] / 2.0\n","\n","        with output_buf.map_to_host() as output_host:\n","            print(\"output:\", output_host)\n","            print(\"expected:\", expected_buf)\n","\n","            # Verify results\n","            for i in range(SIZE):\n","                assert_almost_equal(output_host[i], expected_buf[i], rtol=1e-4)\n","\n","    print(\"✅ Conditional broadcast test passed!\")\n","\n","\n","def main():\n","    print(\"WARP_SIZE: \", WARP_SIZE)\n","    if len(argv()) < 1 or len(argv()) > 2:\n","        print(\n","            \"Usage: p23.mojo\"\n","            \" [--neighbor|--average|--broadcast-basic|--broadcast-conditional|--broadcast-shuffle-coordination]\"\n","        )\n","        return\n","\n","    test_type = argv()[1]\n","    if test_type == \"--neighbor\":\n","        print(\"SIZE: \", SIZE)\n","        test_neighbor_difference()\n","    elif test_type == \"--average\":\n","        print(\"SIZE_2: \", SIZE_2)\n","        test_moving_average()\n","    elif test_type == \"--broadcast-basic\":\n","        print(\"SIZE: \", SIZE)\n","        test_basic_broadcast()\n","    elif test_type == \"--broadcast-conditional\":\n","        print(\"SIZE: \", SIZE)\n","        test_conditional_broadcast()\n","    elif test_type == \"--broadcast-shuffle-coordination\":\n","        print(\"SIZE: \", SIZE)\n","        test_broadcast_shuffle_coordination()\n","    else:\n","        print(\n","            \"Usage: p23.mojo\"\n","            \" [--neighbor|--average|--broadcast-basic|--broadcast-conditional|--broadcast-shuffle-coordination]\"\n","        )\n","\"\"\""],"metadata":{"id":"Bnz0bVKGHurK","executionInfo":{"status":"ok","timestamp":1756753504542,"user_tz":-330,"elapsed":68,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p25/p25.mojo\")"],"metadata":{"id":"qB7uVufaH1uB","executionInfo":{"status":"ok","timestamp":1756753504804,"user_tz":-330,"elapsed":16,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!cd /content/mojo-gpu-puzzles && uv run poe p25 --broadcast-basic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KQQXNB1H1wC","executionInfo":{"status":"ok","timestamp":1756753553980,"user_tz":-330,"elapsed":1417,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"fd5e984c-8580-4e29-b77f-ea119bfb3d54"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo problems/p25/p25.mojo --broadcast-basic\u001b[0m\n","WARP_SIZE:  32\n","SIZE:  32\n","output: HostBuffer([11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0])\n","expected: HostBuffer([11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0])\n","✅ Basic broadcast test passed!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5_EKUnDOIbSs","executionInfo":{"status":"ok","timestamp":1756753164180,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":10,"outputs":[]}]}