{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMukScIQIJmExU8Cmj9Bpb2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e89cQXntcPkb","executionInfo":{"status":"ok","timestamp":1753822437875,"user_tz":-330,"elapsed":29070,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"e0bf21db-6884-4678-e87e-8ecb1e0a030f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n","Collecting max\n","  Downloading https://dl.modular.com/public/nightly/python/max-25.4.0-py3-none-manylinux_2_34_x86_64.whl (285.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.0/285.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from max) (8.2.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from max) (2.0.2)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.11/dist-packages (from max) (4.67.1)\n","Installing collected packages: max\n","Successfully installed max-25.4.0\n"]}],"source":["!pip install max --index-url https://dl.modular.com/public/nightly/python/simple/"]},{"cell_type":"code","source":["!git clone https://github.com/modular/mojo-gpu-puzzles"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdUTHSSAehHn","executionInfo":{"status":"ok","timestamp":1753822444074,"user_tz":-330,"elapsed":6200,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"3f3153d4-c928-4dcd-fc65-72d145353374"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mojo-gpu-puzzles'...\n","remote: Enumerating objects: 5684, done.\u001b[K\n","remote: Counting objects: 100% (803/803), done.\u001b[K\n","remote: Compressing objects: 100% (258/258), done.\u001b[K\n","remote: Total 5684 (delta 647), reused 629 (delta 519), pack-reused 4881 (from 2)\u001b[K\n","Receiving objects: 100% (5684/5684), 101.45 MiB | 27.38 MiB/s, done.\n","Resolving deltas: 100% (3559/3559), done.\n"]}]},{"cell_type":"code","source":["!curl -fsSL https://astral.sh/uv/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5eoaypbei_1","executionInfo":{"status":"ok","timestamp":1753822445896,"user_tz":-330,"elapsed":1820,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"b579c4ce-202e-4f9e-d9fc-b0e1dea72d5a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading uv 0.8.3 x86_64-unknown-linux-gnu\n","no checksums to verify\n","installing to /usr/local/bin\n","  uv\n","  uvx\n","everything's installed!\n"]}]},{"cell_type":"code","source":["import max.support.notebook"],"metadata":{"id":"ebM9xkoZepVV","executionInfo":{"status":"ok","timestamp":1753822445908,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def save_code_to_file(text: str, filename: str):\n","    with open(filename, 'w', encoding='utf-8') as file:\n","        file.write(text)"],"metadata":{"id":"VVU1xnpVerM0","executionInfo":{"status":"ok","timestamp":1753822445916,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["mojo_code = \"\"\"\n","from gpu import thread_idx, block_idx, block_dim, barrier\n","from gpu.host import DeviceContext\n","from layout import Layout, LayoutTensor\n","from layout.tensor_builder import LayoutTensorBuild as tb\n","from sys import sizeof, argv\n","from math import log2\n","from testing import assert_equal\n","\n","# ANCHOR: prefix_sum_simple\n","alias TPB = 8\n","alias SIZE = 8\n","alias BLOCKS_PER_GRID = (1, 1)\n","alias THREADS_PER_BLOCK = (TPB, 1)\n","alias dtype = DType.float32\n","alias layout = Layout.row_major(SIZE)\n","\n","\n","fn prefix_sum_simple[\n","    layout: Layout\n","](\n","    output: LayoutTensor[mut=False, dtype, layout],\n","    a: LayoutTensor[mut=False, dtype, layout],\n","    size: Int,\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","    # FILL ME IN (roughly 18 lines) ###################### SIMPLE ######################\n","    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n","    if global_i < SIZE:\n","      shared[local_i] = a[global_i]\n","\n","    barrier()\n","\n","    ####### logic here ######\n","\n","    offset = 1\n","    for i in range(Int(log2(Scalar[dtype](TPB)))):\n","      var current_val: output.element_type = 0\n","      if local_i >= offset and local_i < size:\n","        current_val = shared[local_i - offset]\n","\n","      barrier()\n","      if local_i >= offset and local_i < size:\n","        shared[local_i] += current_val\n","\n","      barrier()\n","      offset *= 2\n","\n","    if global_i < size:\n","      output[global_i]  = shared[local_i]\n","\n","\n","# ANCHOR_END: prefix_sum_simple\n","\n","# ANCHOR: prefix_sum_complete\n","alias SIZE_2 = 15\n","alias BLOCKS_PER_GRID_2 = (2, 1)\n","alias THREADS_PER_BLOCK_2 = (TPB, 1)\n","alias EXTENDED_SIZE = SIZE_2 + 2  # up to 2 blocks\n","alias extended_layout = Layout.row_major(EXTENDED_SIZE)\n","\n","\n","# Kernel 1: Compute local prefix sums and store block sums in out\n","fn prefix_sum_local_phase[\n","    out_layout: Layout, in_layout: Layout\n","](\n","    output: LayoutTensor[mut=False, dtype, out_layout],\n","    a: LayoutTensor[mut=False, dtype, in_layout],\n","    size: Int,\n","):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    local_i = thread_idx.x\n","    # FILL ME IN (roughly 20 lines) ################## COMPLETE ###################\n","    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n","    if global_i < size:\n","      shared[local_i] = a[global_i]\n","\n","    barrier()\n","\n","    ##### logic here #####\n","    offset = 1\n","    for i in range(Int(log2(Scalar[dtype](TPB)))):\n","      var current_val: output.element_type = 0\n","      if local_i >= offset and local_i < TPB:\n","        current_val = shared[local_i - offset]\n","\n","      barrier()\n","\n","      if local_i >= offset and local_i < TPB:\n","        shared[local_i] += current_val\n","\n","      barrier()\n","      offset *= 2\n","\n","    if global_i < size:\n","      output[global_i] = shared[local_i]\n","\n","    if local_i == TPB -1:\n","      output[size + block_idx.x] = shared[local_i]\n","\n","\n","# Kernel 2: Add block sums to their respective blocks\n","fn prefix_sum_block_sum_phase[\n","    layout: Layout\n","](output: LayoutTensor[mut=False, dtype, layout], size: Int):\n","    global_i = block_dim.x * block_idx.x + thread_idx.x\n","    # FILL ME IN (roughly 3 lines)\n","    if block_idx.x > 0 and global_i < size:\n","      prev_block_sum  = output[size + block_idx.x  - 1]\n","      output[global_i] += prev_block_sum\n","\n","# ANCHOR_END: prefix_sum_complete\n","\n","\n","def main():\n","    with DeviceContext() as ctx:\n","        if len(argv()) != 2 or argv()[1] not in [\n","            \"--simple\",\n","            \"--block-boundary\",\n","        ]:\n","            raise Error(\n","                \"Expected one command-line argument: '--simple' or\"\n","                \" '--block-boundary'\"\n","            )\n","\n","        use_simple = argv()[1] == \"--simple\"\n","\n","        size = SIZE if use_simple else SIZE_2\n","        num_blocks = (size + TPB - 1) // TPB\n","\n","        if not use_simple and num_blocks > EXTENDED_SIZE - SIZE_2:\n","            raise Error(\"Extended buffer too small for the number of blocks\")\n","\n","        buffer_size = size if use_simple else EXTENDED_SIZE\n","        out = ctx.enqueue_create_buffer[dtype](buffer_size).enqueue_fill(0)\n","        a = ctx.enqueue_create_buffer[dtype](size).enqueue_fill(0)\n","\n","        with a.map_to_host() as a_host:\n","            for i in range(size):\n","                a_host[i] = i\n","\n","        a_tensor = LayoutTensor[mut=False, dtype, layout](a.unsafe_ptr())\n","\n","        if use_simple:\n","            out_tensor = LayoutTensor[mut=False, dtype, layout](\n","                out.unsafe_ptr()\n","            )\n","\n","            ctx.enqueue_function[prefix_sum_simple[layout]](\n","                out_tensor,\n","                a_tensor,\n","                size,\n","                grid_dim=BLOCKS_PER_GRID,\n","                block_dim=THREADS_PER_BLOCK,\n","            )\n","        else:\n","            var out_tensor = LayoutTensor[mut=False, dtype, extended_layout](\n","                out.unsafe_ptr()\n","            )\n","\n","            # ANCHOR: prefix_sum_complete_block_level_sync\n","            # Phase 1: Local prefix sums\n","            ctx.enqueue_function[\n","                prefix_sum_local_phase[extended_layout, extended_layout]\n","            ](\n","                out_tensor,\n","                a_tensor,\n","                size,\n","                grid_dim=BLOCKS_PER_GRID_2,\n","                block_dim=THREADS_PER_BLOCK_2,\n","            )\n","\n","            # Wait for all `blocks` to complete with using host `ctx.synchronize()`\n","            # Note this is in contrast with using `barrier()` in the kernel\n","            # which is a synchronization point for all threads in the same block and not across blocks.\n","            ctx.synchronize()\n","\n","            # Phase 2: Add block sums\n","            ctx.enqueue_function[prefix_sum_block_sum_phase[extended_layout]](\n","                out_tensor,\n","                size,\n","                grid_dim=BLOCKS_PER_GRID_2,\n","                block_dim=THREADS_PER_BLOCK_2,\n","            )\n","            # ANCHOR_END: prefix_sum_complete_block_level_sync\n","\n","        # Verify results for both cases\n","        expected = ctx.enqueue_create_host_buffer[dtype](size).enqueue_fill(0)\n","        ctx.synchronize()\n","\n","        with a.map_to_host() as a_host:\n","            expected[0] = a_host[0]\n","            for i in range(1, size):\n","                expected[i] = expected[i - 1] + a_host[i]\n","\n","        with out.map_to_host() as out_host:\n","            if not use_simple:\n","                print(\n","                    \"Note: we print the extended buffer here, but we only need\"\n","                    \" to print the first `size` elements\"\n","                )\n","\n","            print(\"out:\", out_host)\n","            print(\"expected:\", expected)\n","            # Here we need to use the size of the original array, not the extended one\n","            size = size if use_simple else SIZE_2\n","            for i in range(size):\n","                assert_equal(out_host[i], expected[i])\n","\"\"\""],"metadata":{"id":"ROrG97Yves-Z","executionInfo":{"status":"ok","timestamp":1753822445942,"user_tz":-330,"elapsed":25,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["save_code_to_file(mojo_code, \"/content/mojo-gpu-puzzles/problems/p12/p12.mojo\")"],"metadata":{"id":"s9KWFSFAey4J","executionInfo":{"status":"ok","timestamp":1753822445945,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!cd /content/mojo-gpu-puzzles && uv run poe p12 --block-boundary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjwLtSWZey74","executionInfo":{"status":"ok","timestamp":1753822769295,"user_tz":-330,"elapsed":2415,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}},"outputId":"22b1d668-a92a-400a-feae-3754824bee67"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[37mPoe =>\u001b[0m \u001b[94mmojo problems/p12/p12.mojo --block-boundary\u001b[0m\n","Note: we print the extended buffer here, but we only need to print the first `size` elements\n","out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0, 28.0, 77.0])\n","expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2U550o1NfG6M","executionInfo":{"status":"ok","timestamp":1753822560626,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mojonized","userId":"01435840102907113887"}}},"execution_count":8,"outputs":[]}]}